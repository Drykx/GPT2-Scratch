{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10fe08a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "\n",
    "from GPT2.hellaswag import render_example, iterate_examples\n",
    "from GPT2.gpt2_functions import *\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86636972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Distributed data parallel (DDP)\n",
    "\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # Is this a ddp run?\n",
    "if ddp:\n",
    "    # DDP requieres CUDA\n",
    "    assert torch.cuda.is_available(), \"We need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "\n",
    "    ddp_rank       = int(os.environ['RANK'])       # Global GPU index\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK']) # Current node GPU index\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE']) # Total number GPUs across nodes\n",
    "    \n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    # Choose process: logging, checkpointing...\n",
    "    master_process = ddp_rank == 0 \n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank       = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    # attempt to autodetect device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f21cff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "no shards found for split train",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m enc = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Loader functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_loader = \u001b[43mDataLoaderLite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddp_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddp_world_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m val_loader   = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m  )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\GPT2\\gpt2_functions.py:54\u001b[39m, in \u001b[36mDataLoaderLite.__init__\u001b[39m\u001b[34m(self, B, T, process_rank, num_processes, split)\u001b[39m\n\u001b[32m     52\u001b[39m shards = [os.path.join(data_root, s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shards]   \u001b[38;5;66;03m# [\"train_000.npy\"] -> [\"edu_fineweb10B/train_000.npy\"]\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.shards = shards\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shards) > \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mno shards found for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m master_process:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(shards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m shards for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: no shards found for split train"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Define precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Hyperparameters\n",
    "total_batch_size = 524288 # 2**19 (close to 0.5M)\n",
    "B                = 16     # Micro Batch Size (use gradient accumulation)\n",
    "T                = 1024   # Sequence Length\n",
    "vocab_size       = 50304 \n",
    "max_lr           = 6e-4\n",
    "min_lr           = max_lr * 0.1\n",
    "warmup_steps     = 715\n",
    "max_steps        = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "weight_decay     = 0.1\n",
    "learning_rate    = 6e-4\n",
    "\n",
    "# Grad accum step\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"Make sure total batch size is div by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "\n",
    "# Encoder\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Loader functions\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
    "val_loader   = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\"  )\n",
    "\n",
    "# Model\n",
    "model = GPT2(GPT2Config(vocab_size=vocab_size))\n",
    "model.to(device)    # Move to device\n",
    "use_compile = False # torch.compile infers with HellaSwag\n",
    "if use_compile:\n",
    "    model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids = [ddp_local_rank])\n",
    "raw_model = model.module if ddp else model \n",
    "\n",
    "# Optimize\n",
    "optimizer = raw_model.configure_optimizers(\n",
    "    weight_decay  = weight_decay,\n",
    "    learning_rate = learning_rate,\n",
    "    device_type   = device_type\n",
    ")\n",
    "\n",
    "# print\n",
    "if master_process: \n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"-> Calculated grad acc steps is {grad_accum_steps}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log directory\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # Every 250 steps evaluate validation loss for 20 steps\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op = dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "            if step > 0 and (step % 5000 == 0 or last_step):\n",
    "                # optionally write model checkpoints\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Every 250 steps evaluate hellaswag\n",
    "    if (step % 250 == 0 or last_step) and (not use_compile):\n",
    "        num_correct_norm = 0\n",
    "        num_total = 0\n",
    "        for i, example in enumerate(iterate_examples(\"val\")):\n",
    "            # only process examples where i % ddp_world_size == ddp_rank\n",
    "            if i % ddp_world_size != ddp_rank:\n",
    "                continue\n",
    "            # render the example into tokens and labels\n",
    "            _, tokens, mask, label = render_example(example)\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            # get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(tokens)\n",
    "                pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "            num_total += 1\n",
    "            num_correct_norm += int(pred_norm == label)\n",
    "        # reduce the stats across all processes\n",
    "        if ddp:\n",
    "            num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
    "            num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
    "            dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
    "            dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
    "            num_total = num_total.item()\n",
    "            num_correct_norm = num_correct_norm.item()\n",
    "        acc_norm = num_correct_norm / num_total\n",
    "        if master_process:\n",
    "            print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        # we have to scale the loss to account for gradient accumulation,\n",
    "        # because the gradients just add on each successive backward().\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
