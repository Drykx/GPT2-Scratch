# GPT2-Scratch

## ðŸ“š Overview

This project is a personal implementation of GPT-2 from scratch using PyTorch. It reflects my effort to gain a deeper understanding of the inner workings of large language models (LLMs).

To support this learning journey, I relied on two excellent resources:

- ðŸ“– [*Deep Learning: Foundation and Concepts* by Christopher M. Bishop](https://www.bishopbook.com/) â€“ Chapters 6â€“13
- ðŸŽ¥ [Neural Networks: Zero to Hero by Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)

In my experience, these are among the most comprehensive and effective learning materials available. I highly recommend them to both beginners and intermediate learners. Even if youâ€™ve previously worked with LLMs, Karpathyâ€™s clear explanations and hands-on approach offer valuable insights that bridge theory and implementation.

## ðŸ§  Goals

- Deepen understanding of transformer architecture to explore improvements  
- Rebuild GPT-2 components to sharpen coding skills  
- Practice turning research papers into code  
- Learn best practices from expert implementations  
- Strengthen debugging and scaling from first principles  

## ðŸ”§ Tools Used

- Python & PyTorch  
- Hugging Face Datasets (for experimentation)  
- AWS (for compute resources)  

## Structure Repo

â”œâ”€â”€ Makemore/                   # Character-level bigram model with scalable extensions to reduce training loss
â”‚
â”œâ”€â”€ Tokenizer/                  # Custom Byte Pair Encoding (BPE) tokenizer implementation
â”‚
â”œâ”€â”€ GPT/                        # Lightweight GPT model for generating French poetry using the custom tokenizer
â”‚
â”œâ”€â”€ GP2T/                       # Scalable GPT-2-style model trained on 10B tokens; evaluated on HellaSwag using GPT-2 tiktoken encoding
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies and environment setup
