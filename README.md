# GPT2-Scratch

## ðŸ“š Overview

This project is a personal implementation of GPT-2 from scratch using PyTorch. It reflects my effort to gain a deeper understanding of the inner workings of large language models (LLMs).

To support this learning journey, I relied on two excellent resources:

- ðŸ“– [*Deep Learning: Foundation and Concepts* by Christopher M. Bishop](https://www.bishopbook.com/) â€“ Chapters 6â€“13
- ðŸŽ¥ [Neural Networks: Zero to Hero by Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)

In my experience, these are among the most comprehensive and effective learning materials available. I highly recommend them to both beginners and intermediate learners. Even if youâ€™ve previously worked with LLMs, Karpathyâ€™s clear explanations and hands-on approach offer valuable insights that bridge theory and implementation.

## ðŸ§  Goals

- Deepen understanding of transformer architecture to explore improvements  
- Rebuild GPT-2 components to sharpen coding skills  
- Practice turning research papers into code  
- Learn best practices from expert implementations  
- Strengthen debugging and scaling from first principles  

## ðŸ”§ Tools Used

- Python & PyTorch  
- Hugging Face Datasets (for experimentation)  
- AWS (for compute resources)  

---

Stay tuned for future updates, including training logs, evaluation metrics, and further model refinements.

