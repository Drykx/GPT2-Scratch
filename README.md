# GPT2-Scratch

## ðŸ“š Overview

This project is a personal implementation of GPT-2 from scratch using PyTorch. It reflects my effort to gain a deeper understanding of the inner workings of large language models (LLMs).

To support this learning journey, I relied on two excellent resources:

- ðŸ“– [*Pattern Recognition and Machine Learning* by Christopher M. Bishop](https://www.bishopbook.com/) â€“ Chapters 6â€“13
- ðŸŽ¥ [Neural Networks: Zero to Hero by Andrej Karpathy](https://www.youtube.com/playlist?list=PLpPXw4zFa0uJ6DfbMNaMh2-1YfMcXj7oH)

In my experience, these are among the most comprehensive and effective learning materials available. I highly recommend them to both beginners and intermediate learners. Even if youâ€™ve previously worked with LLMs, Karpathyâ€™s clear explanations and hands-on approach offer valuable insights that bridge theory and implementation.

## ðŸ§  Goals

- Understand transformer architecture at a low level  
- Build all key GPT-2 components manually  
- Train a simple GPT-2-like model on real text data  
- Practice debugging and scaling ML models from first principles  

## ðŸ”§ Tools Used

- Python & PyTorch  
- Jupyter Notebooks  
- Hugging Face Datasets (for experimentation)  
- AWS (for compute resources)  

---

Stay tuned for future updates, including training logs, evaluation metrics, and further model refinements.

