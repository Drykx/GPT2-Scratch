# GPT2-Scratch

## ðŸ“š Overview

This project is a personal implementation of GPT-2 from scratch using PyTorch. It reflects my effort to gain a deeper understanding of the inner workings of large language models (LLMs).

To support this learning journey, I relied on two excellent resources:

- ðŸ“– [*Deep Learning: Foundation and Concepts* by Christopher M. Bishop](https://www.bishopbook.com/) â€“ Chapters 6â€“13
- ðŸŽ¥ [Neural Networks: Zero to Hero by Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)

In my experience, these are among the most comprehensive and effective learning materials available. I highly recommend them to both beginners and intermediate learners. Even if youâ€™ve previously worked with LLMs, Karpathyâ€™s clear explanations and hands-on approach offer valuable insights that bridge theory and implementation.

## ðŸ§  Goals

- Understand transformer architecture at a low level in hope to improve it
- Build all key GPT-2 components manually to improve my coding
- Train to reply research papers into code
- Insipire and learn from industry experts for the good practices
- Practice debugging and scaling ML models from first principles

## ðŸ”§ Tools Used

- Python & PyTorch  
- Hugging Face Datasets (for experimentation)  
- AWS (for compute resources)  

---

Stay tuned for future updates, including training logs, evaluation metrics, and further model refinements.

