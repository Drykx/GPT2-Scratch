{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fe08a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtiktoken\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m \n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m \n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\__init__.py:2475\u001b[39m\n\u001b[32m   2471\u001b[39m     torch_module_name = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.join([\u001b[34m__name__\u001b[39m, device_type])\n\u001b[32m   2472\u001b[39m     sys.modules[torch_module_name] = module\n\u001b[32m-> \u001b[39m\u001b[32m2475\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2476\u001b[39m     export \u001b[38;5;28;01mas\u001b[39;00m export,\n\u001b[32m   2477\u001b[39m     func \u001b[38;5;28;01mas\u001b[39;00m func,\n\u001b[32m   2478\u001b[39m     library \u001b[38;5;28;01mas\u001b[39;00m library,\n\u001b[32m   2479\u001b[39m     return_types \u001b[38;5;28;01mas\u001b[39;00m return_types,\n\u001b[32m   2480\u001b[39m )\n\u001b[32m   2481\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m cond, while_loop \u001b[38;5;28;01mas\u001b[39;00m while_loop\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vmap \u001b[38;5;28;01mas\u001b[39;00m vmap\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\export\\__init__.py:64\u001b[39m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msymbolic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrictMinMaxConstraint\n\u001b[32m     44\u001b[39m __all__ = [\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConstraint\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDim\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnflattenedModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_shapes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Constraint, Dim, dims, ShapesCollection\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexported_program\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram, ModuleCallEntry, ModuleCallSignature\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_signature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportBackwardSignature, ExportGraphSignature\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\export\\dynamic_shapes.py:23\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     12\u001b[39m     _get_node_type,\n\u001b[32m     13\u001b[39m     BUILTIN_TYPES,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     tree_map_with_path,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexported_program\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExportedProgram\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msympy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Symbol\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\export\\exported_program.py:26\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     Any,\n\u001b[32m     14\u001b[39m     Callable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     Union,\n\u001b[32m     24\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m autograd_not_implemented\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_class_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeScriptObject\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m first_call_function_nn_module_stack\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcond\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflex_attention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     flex_attention,\n\u001b[32m      4\u001b[39m     flex_attention_backward,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_higher_order_ops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhints_wrap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hints_wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional_tensor\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, ContextManager, Dict, List, Optional, Tuple, Union\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_inductor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minductor_config\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pytree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytree\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _functionalization_reapply_views_tls \u001b[38;5;28;01mas\u001b[39;00m _reapply_views\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\_inductor\\config.py:44\u001b[39m\n\u001b[32m     40\u001b[39m verbose_progress = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# use fx aot graph codegen cache\u001b[39;00m\n\u001b[32m     43\u001b[39m fx_graph_cache = (\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mTORCHINDUCTOR_FX_GRAPH_CACHE\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_fbcode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     45\u001b[39m )\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# use remote fx aot graph codegen cache\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# False: Disables the cache\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# True: Enables the cache\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# None: Not set -- Off for OSS, JustKnobs based for internal\u001b[39;00m\n\u001b[32m     51\u001b[39m fx_graph_remote_cache: Optional[\u001b[38;5;28mbool\u001b[39m] = fx_graph_remote_cache_default()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jean-\\OneDrive\\Bureau\\Projects\\Personnal\\GPT2-Scratch\\.venv\\Lib\\site-packages\\torch\\_inductor\\config.py:9\u001b[39m, in \u001b[36mis_fbcode\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_fbcode\u001b[39m() -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mgit_version\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'version' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "import wget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset\n",
    "\n",
    "from hellaswag import render_example, iterate_examples\n",
    "\n",
    "torch.manual_seed(42)\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdabba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.venv', '.vscode', 'classes.py', 'gpt.ipynb', 'gpt2.ipynb', 'gpt_functions.py', 'hellaswag.py', 'input.txt', 'makemore.ipynb', 'names.txt', 'notebook.ipynb', 'README.md', 'requirements.txt', 'tokenizer.ipynb', 'tokenizer_classes.py', 'tokenizer_functions.py', 'utils.py', '__pycache__']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.listdir(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f3144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Config GPT2\n",
    "# -------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    block_size: int = 1024  # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int    = 12    # number of layers\n",
    "    n_head: int     = 12    # number of heads\n",
    "    n_embd: int     = 768   # embedding dimension\n",
    "    \n",
    "# -------------------------------------\n",
    "# Attention Block\n",
    "# -------------------------------------\n",
    "\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh: \"number of heads\", hs: \"head size\", and C: \"number of channels\" (nh * hs)\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)     # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)                # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "# -------------------------------------\n",
    "# MLP\n",
    "# -------------------------------------\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x) \n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------------\n",
    "# Transformer Block\n",
    "# -------------------------------------\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" \n",
    "    Transformer block: \n",
    "    Applies LayerNorm → CausalSelfAttention → residual add, \n",
    "    then LayerNorm → MLP → residual add.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "# -------------------------------------\n",
    "# General GPT2 \n",
    "# -------------------------------------\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),              # Word-Token Embedding\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),              # Word-Position Embedding\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Multi-Head Attention\n",
    "            ln_f = nn.LayerNorm(config.n_embd),                                # Layer Normalization\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # Finish with a Linear-Head\n",
    "\n",
    "        # Weight sharing scheme: inking references (reduces the number of parameters by 30%) \n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # Init params (apply _init_weights to every submodule)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize parameters of the model: cautious variance of residual connections!\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                # Stabilize variance (sqrt(2* number_residual_paths))\n",
    "                # Per block there is 2 (Attention-Head + MLP)\n",
    "                std *= (2 * self.config.n_layer) ** -0.5 \n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Classical forward layer\"\"\"\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # Forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod \n",
    "    def from_pretrained(cls,model_type): \n",
    "        \"\"\"Loads pretrained from HF\"\"\"\n",
    "        assert model_type in {\"gpt2\",\"gpt2-medium\",\"gpt2-large\",\"gpt2-xl\"}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"Loading weights from the pretrained gpt2Model:{model_type}\")\n",
    "\n",
    "        config_args = {\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "\n",
    "        config_args['vocab_size'] = 50257 # Config GPT2\n",
    "        config_args['block_size'] = 1024  # Config GPT2\n",
    "\n",
    "        # Initialize a from-scratch minGPT model\n",
    "        config = GPT2Config(**config_args) # Unpack model configuration\n",
    "        model = GPT2(config)               # Create model instance\n",
    "\n",
    "        # Extract state dictionary (learned parameters)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = [k for k in sd if not k.endswith('.attn.bias')]  # Exclude attention bias buffers (not learnable params)\n",
    "\n",
    "        # Init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # Copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore (buffer)\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]        # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        \n",
    "        # The openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # Special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # Vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "    \n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        \"\"\"\n",
    "        Specialize optimizer treating decay and no decay parameters seperatly\n",
    "        \"\"\"\n",
    "        # Dict all params with requires_grad\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        # Distinguish params according to their dimension\n",
    "        decay_params   = [p for n,p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n,p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_day\": 0.0}\n",
    "        ]\n",
    "\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        # Check use: Special CUDA-optimized  fused kernel implementation\n",
    "        \n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        used_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process: \n",
    "            print(f\"Using fused AdamW: {used_fused}\")\n",
    "\n",
    "        # Create AdamW optimizer with hyperparameters\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr = learning_rate, betas = (0.9,0.95), eps = 1e-8, fused = used_fused)\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cde75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Load data and convert it in Pytorch\n",
    "# -------------------------------------\n",
    "\n",
    "def load_tokens(filename):\n",
    "    \"Load and Transform data into a torch tensor\"\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32)                 # Vocabulary size is within int32 range\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)  # Pytorch requires torch.long for indexing\n",
    "    return ptt\n",
    "\n",
    "# -------------------------------------\n",
    "# Shards data and process it in batches\n",
    "# -------------------------------------\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, process_rank, num_processes, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.process_rank = process_rank    # ID current process (0 to process_rank - 1) \n",
    "        self.num_processes = num_processes  # Total number GPUs running in parallel\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        os.makedirs(\"edu_fineweb10B\", exist_ok = True)\n",
    "        data_root = \"edu_fineweb10B\"                            # Root dir\n",
    "        shards = os.listdir(data_root)                          # List dir's files\n",
    "        shards = [s for s in shards if split in s]              # Filter files with split\n",
    "        shards = sorted(shards)                                 # Order shards\n",
    "        shards = [os.path.join(data_root, s) for s in shards]   # [\"train_000.npy\"] -> [\"edu_fineweb10B/train_000.npy\"]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        if master_process:\n",
    "            print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # State, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        # Browse file's token by chunk B*T*Process_rank\n",
    "        self.current_position = self.B * self.T * self.process_rank\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # Inputs\n",
    "        y = (buf[1:]).view(B, T)  # Targets\n",
    "        # Advance the position in the tensor\n",
    "        self.current_position += B * T * self.num_processes\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T * self.process_rank\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# Helper function: HellaSwag\n",
    "# -------------------------------------\n",
    "\n",
    "def get_most_likely_row(tokens, mask, logits):\n",
    "    # Evaluate the autoregressive loss at all positions\n",
    "    shift_logits = (logits[..., :-1, :]).contiguous()\n",
    "    shift_tokens = (tokens[..., 1:]).contiguous()\n",
    "    # Flatten tensors for loss \n",
    "    flat_shift_logits = shift_logits.view(-1, shift_logits.size(-1))                       # (B * (T-1), vocab_size)\n",
    "    flat_shift_tokens = shift_tokens.view(-1)                                              # (B * (T-1))\n",
    "    shift_losses = F.cross_entropy(flat_shift_logits, flat_shift_tokens, reduction='none') # (B * (T-1))\n",
    "    shift_losses = shift_losses.view(tokens.size(0), -1)                                   # (B, (T-1))\n",
    "    # Now get the average loss just for the completion region (where mask == 1), in each row\n",
    "    shift_mask = (mask[..., 1:]).contiguous() # Shift mask, so we start at the last prompt token\n",
    "    masked_shift_losses = shift_losses * shift_mask\n",
    "    # Sum and divide by the number of 1s in the mask\n",
    "    sum_loss = masked_shift_losses.sum(dim=1)\n",
    "    avg_loss = sum_loss / shift_mask.sum(dim=1)\n",
    "    # Now we have a loss for each of the 4 completions\n",
    "    # The one with the lowest loss should be the most likely\n",
    "    pred_norm = avg_loss.argmin().item()\n",
    "    return pred_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86636972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Distributed data parallel (DDP)\n",
    "# -------------------------------------\n",
    "\n",
    "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # Is this a ddp run?\n",
    "if ddp:\n",
    "    # DDP requieres CUDA\n",
    "    assert torch.cuda.is_available(), \"We need CUDA for DDP\"\n",
    "    init_process_group(backend='nccl')\n",
    "\n",
    "    ddp_rank       = int(os.environ['RANK'])       # Global GPU index\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK']) # Current node GPU index\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE']) # Total number GPUs across nodes\n",
    "    \n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    # Choose process: logging, checkpointing...\n",
    "    master_process = ddp_rank == 0 \n",
    "else:\n",
    "    # vanilla, non-DDP run\n",
    "    ddp_rank       = 0\n",
    "    ddp_local_rank = 0\n",
    "    ddp_world_size = 1\n",
    "    master_process = True\n",
    "    # attempt to autodetect device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
    "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
    "\n",
    "# -------------------------------------\n",
    "# Learning Rate function\n",
    "# -------------------------------------\n",
    "\n",
    "def get_lr(it, warmup_steps, max_steps, max_lr, min_lr):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_steps:\n",
    "        return max_lr * (it+1) / warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > max_steps:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d741c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21cff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'edu_fineweb10B'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m enc = tiktoken.get_encoding(\u001b[33m\"\u001b[39m\u001b[33mgpt2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Loader functions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m train_loader = \u001b[43mDataLoaderLite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddp_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mddp_world_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m val_loader   = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m  )\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Model\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mDataLoaderLite.__init__\u001b[39m\u001b[34m(self, B, T, process_rank, num_processes, split)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# get the shard filenames\u001b[39;00m\n\u001b[32m     25\u001b[39m data_root = \u001b[33m\"\u001b[39m\u001b[33medu_fineweb10B\u001b[39m\u001b[33m\"\u001b[39m                            \u001b[38;5;66;03m# Root dir\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m shards = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m)\u001b[49m                          \u001b[38;5;66;03m# List dir's files\u001b[39;00m\n\u001b[32m     27\u001b[39m shards = [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shards \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m s]              \u001b[38;5;66;03m# Filter files with split\u001b[39;00m\n\u001b[32m     28\u001b[39m shards = \u001b[38;5;28msorted\u001b[39m(shards)                                 \u001b[38;5;66;03m# Order shards\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'edu_fineweb10B'"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Define precision\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# Hyperparameters\n",
    "total_batch_size = 524288 # 2**19 (close to 0.5M)\n",
    "B                = 16     # Micro Batch Size (use gradient accumulation)\n",
    "T                = 1024   # Sequence Length\n",
    "vocab_size       = 50304 \n",
    "max_lr           = 6e-4\n",
    "min_lr           = max_lr * 0.1\n",
    "warmup_steps     = 715\n",
    "max_steps        = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
    "weight_decay     = 0.1\n",
    "learning_rate    = 6e-4\n",
    "\n",
    "# Grad accum step\n",
    "assert total_batch_size % (B * T * ddp_world_size) == 0, \"Make sure total batch size is div by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
    "\n",
    "# Encoder\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Loader functions\n",
    "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
    "val_loader   = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\"  )\n",
    "\n",
    "# Model\n",
    "model = GPT2(GPT2Config(vocab_size=vocab_size))\n",
    "model.to(device)    # Move to device\n",
    "use_compile = False # torch.compile infers with HellaSwag\n",
    "if use_compile:\n",
    "    model = torch.compile(model)\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids = [ddp_local_rank])\n",
    "raw_model = model.module if ddp else model \n",
    "\n",
    "# Optimize\n",
    "optimizer = raw_model.configure_optimizers(\n",
    "    weight_decay  = weight_decay,\n",
    "    learning_rate = learning_rate,\n",
    "    device_type   = device_type\n",
    ")\n",
    "\n",
    "# print\n",
    "if master_process: \n",
    "    print(f\"total desired batch size: {total_batch_size}\")\n",
    "    print(f\"-> Calculated grad acc steps is {grad_accum_steps}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log directory\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # Every 250 steps evaluate validation loss for 20 steps\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "        if ddp:\n",
    "            dist.all_reduce(val_loss_accum, op = dist.ReduceOp.AVG)\n",
    "        if master_process:\n",
    "            print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "            if step > 0 and (step % 5000 == 0 or last_step):\n",
    "                # optionally write model checkpoints\n",
    "                checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'config': raw_model.config,\n",
    "                    'step': step,\n",
    "                    'val_loss': val_loss_accum.item()\n",
    "                }\n",
    "                torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Every 250 steps evaluate hellaswag\n",
    "    if (step % 250 == 0 or last_step) and (not use_compile):\n",
    "        num_correct_norm = 0\n",
    "        num_total = 0\n",
    "        for i, example in enumerate(iterate_examples(\"val\")):\n",
    "            # only process examples where i % ddp_world_size == ddp_rank\n",
    "            if i % ddp_world_size != ddp_rank:\n",
    "                continue\n",
    "            # render the example into tokens and labels\n",
    "            _, tokens, mask, label = render_example(example)\n",
    "            tokens = tokens.to(device)\n",
    "            mask = mask.to(device)\n",
    "            # get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(tokens)\n",
    "                pred_norm = get_most_likely_row(tokens, mask, logits)\n",
    "            num_total += 1\n",
    "            num_correct_norm += int(pred_norm == label)\n",
    "        # reduce the stats across all processes\n",
    "        if ddp:\n",
    "            num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
    "            num_correct_norm = torch.tensor(num_correct_norm, dtype=torch.long, device=device)\n",
    "            dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
    "            dist.all_reduce(num_correct_norm, op=dist.ReduceOp.SUM)\n",
    "            num_total = num_total.item()\n",
    "            num_correct_norm = num_correct_norm.item()\n",
    "        acc_norm = num_correct_norm / num_total\n",
    "        if master_process:\n",
    "            print(f\"HellaSwag accuracy: {num_correct_norm}/{num_total}={acc_norm:.4f}\")\n",
    "            with open(log_file, \"a\") as f:\n",
    "                f.write(f\"{step} hella {acc_norm:.4f}\\n\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
    "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        # we have to scale the loss to account for gradient accumulation,\n",
    "        # because the gradients just add on each successive backward().\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    if ddp:\n",
    "        dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0 # time difference in seconds\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
    "    tokens_per_sec = tokens_processed / dt\n",
    "    if master_process:\n",
    "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e75250",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
